% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

%\documentclass{acm_proc_article-sp}
% Camera ready version needs this:
\documentclass{sig-alternate}


\usepackage{color}
\usepackage[ampersand]{easylist}
\usepackage{times}
%\usepackage{mathtime}
\usepackage{booktabs}

\usepackage{listings}
\lstloadlanguages{C++,Pascal}

% Settings for the lstlistings environment
\lstset{
language=C++,                       % choose the language of the code
basicstyle=\footnotesize\ttfamily,  % the size of the fonts that are used for the
                                    % code
numbers=none,                       % where to put the line-numbers
numberstyle=\tiny,                  % the size of the fonts that are used for the
                                    % line-numbers
stepnumber=1,                       % the step between two line-numbers. If it's
                                    % 1 each line will be numbered
numbersep=5pt,                      % how far the line-numbers are from the code
%backgroundcolor=\color{gray},      % choose the background color. You must add
                                    % \usepackage{color}
showspaces=false,                   % show spaces adding particular underscores
showstringspaces=false,             % underline spaces within strings
showtabs=false,                     % show tabs within strings adding particular
                                    % underscores
keywordstyle=\bfseries\color{darkblue},  % color of the keywords
commentstyle=\color{darkgreen},     % color of the comments
stringstyle=\color{darkred},        % color of strings
captionpos=b,                       % sets the caption-position to top
tabsize=2,                          % sets default tabsize to 2 spaces
frame=tb,                       % adds a frame around the code
breaklines=true,                    % sets automatic line breaking
breakatwhitespace=false,            % sets if automatic breaks should only happen
                                    % at whitespace
escapechar=\%,                      % toggles between regular LaTeX and listing
belowskip=0.3cm,                    % vspace after listing
morecomment=[s][\bfseries\color{darkblue}]{struct}{\ },
morecomment=[s][\bfseries\color{darkblue}]{class}{\ },
morecomment=[s][\bfseries\color{darkblue}]{public:}{\ },
morecomment=[s][\bfseries\color{darkblue}]{public}{\ },
morecomment=[s][\bfseries\color{darkblue}]{protected:}{\ },
morecomment=[s][\bfseries\color{darkblue}]{private:}{\ },
morecomment=[s][\bfseries\color{black}]{operator+}{\ },
xleftmargin=0.1cm,
%xrightmargin=0.1cm,
}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\F}[1]{\B{\textcolor{red}{FIXME: #1}}}

\newcommand{\ctype}[1]{\texttt{{{#1}}}}
\newcommand{\cvar}[1]{\texttt{{{#1}}}}
\newcommand{\cfunc}[1]{\texttt{{{#1}()}}}

\newcommand{\BI}[1]{\B{\I{#1}}}
\newcommand{\TI}[1]{\T{\I{#1}}}
\newcommand{\BT}[1]{\B{\T{#1}}}

\newcommand{\upup}{\vspace*{-0.5em}}
\newcommand{\upp}{\vspace*{-0.5em}}
\newcommand{\up}{\vspace*{-0.25em}}


\newcommand{\dnn}{\vspace*{0.5em}}
\newcommand{\dn}{\vspace*{0.25em}}

\hyphenation{op-tical net-works semi-conduc-tor Auto--paral-lelizing paral-lelizing}

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

\permission{
Permission to make digital or hard copies of all or part of this work for personal or classroom use is
granted without fee provided that copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page. Copyrights for components of this
work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
\conferenceinfo{ScalA '13}{November 17-21, 2013, Denver CO, USA}
\CopyrightYear{2013}
\copyrightetc{
    Copyright is held by the owner/author(s). Publication rights licensed to ACM. \\
    ACM 978-1-4503-2508-0/13/11...\$15.00. \\
    http://dx.doi.org/10.1145/2530268.2530269}}

\begin{document}

\setlength{\parskip}{0.3em}

\definecolor{darkblue}{rgb}{0,0,.6}
\definecolor{darkred}{rgb}{.6,0,0}
\definecolor{darkgreen}{rgb}{0,.6,0}
\definecolor{red}{rgb}{.98,0,0}
\definecolor{gray}{rgb}{.6,.6,.6}

% \title{Application Scaling Beyond Conventional Margins\\
%     \vspace*{0.3em}\large{Using HPX and LibGeoDecomp for Extreme Scale on Heterogeneous Architectures}}

\title{Using HPX and LibGeoDecomp for Scaling HPC Applications on Heterogeneous Supercomputers}

%\subtitle{[Extended Abstract]*
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}


\numberofauthors{4}
\author{
% \alignauthor Thomas Heller \\
%   \affaddr{Chair of Computer Science 3, Computer Archictectures, Friedrich-Alexander-University, Erlangen, Germany}\\
% %  \affaddr{Friedrich-Alexander-University, Germany}\\
%   \email{thomas.heller@cs.fau.de}
% \alignauthor Hartmut Kaiser \\
%    \affaddr{Center for Computation and Technology,}
%    \affaddr{Louisiana State University, Louisiana, U.S.A.}
%    \email{hkaiser@cct.lsu.edu}
% \alignauthor Andreas Sch\"afer, Dietmar Fey \\
%   \affaddr{Chair of Computer Science 3, Computer Archictectures, Friedrich-Alexander-University, Erlangen, Germany}\\
% %  \affaddr{Friedrich-Alexander-University, Germany}\\
%   \email{[andreas.schaefer,dietmar.fey]\\@cs.fau.de}
    \alignauthor Thomas Heller\textsuperscript{1} \\
    \email{thomas.heller@cs.fau.de}
    \alignauthor Hartmut Kaiser\textsuperscript{2}
    \email{hkaiser@cct.lsu.edu}
    \alignauthor Andreas Sch\"afer\textsuperscript{1}
    \email{andreas.schaefer@cs.fau.de}
    \and
    \alignauthor
    \alignauthor Dietmar Fey\textsuperscript{1}
    \email{dietmar.fey@cs.fau.de}
    \alignauthor
    \sharedaffiliation
    \affaddr{\textsuperscript{1}Chair of Computer Science 3, Computer Architectures,}\\
    \affaddr{Friedrich-Alexander-University, Erlangen, Germany}\\
    \sharedaffiliation
    \affaddr{\textsuperscript{2}Center for Computation and Technology,} \\
    \affaddr{Louisiana State University, Louisiana, U.S.A.}
}

\maketitle

\begin{abstract}
With the general availability of PetaFLOP clusters and the advent of heterogeneous machines equipped with
special accelerator cards such as
the Xeon Phi\cite{xeon_phi_webpage}, computer scientist face the
difficult task of improving application scalability beyond what is possible with conventional
techniques and programming models today. In addition, the need for highly adaptive runtime algorithms
and for applications handling highly inhomogeneous data further impedes our ability to
efficiently write code which performs and scales well.

%\upp\up
In this paper we present the advantages of using HPX\cite{scaling_impaired_apps,hpx_git,stellar_link}, a general purpose parallel runtime system
for applications of any scale as a backend for LibGeoDecomp\cite{libgeodecomp} for implementing
a three-dimensional N-Body simulation with local interactions. We compare scaling and performance results for this application
while using the HPX and MPI backends for LibGeoDecomp.
LibGeoDecomp is a Library for Geometric Decomposition codes
implementing the idea of a user supplied simulation
model, where the library handles the spatial and temporal loops, and the data storage.
%This enables LibGeoDecomp to handle parallelization,
%accelerator off-loading and parallel IO. The library was originally
%geared towards stencil codes only, but does also support N-body codes
%via a boxing-scheme. A generic, hierarchical parallelization can be
%tuned on each level towards the parallel system at hands.

%\upp\up
The presented results
are acquired from various homogeneous and heterogeneous runs including up to 1024 nodes (16384 conventional cores)
combined with up to 16 Xeon Phi accelerators (3856 hardware threads) on TACC's Stampede supercomputer\cite{stampede_tacc}.
In the configuration using the HPX backend, more than 0.35 PFLOPS have been achieved, which
corresponds to a parallel application efficiency of around 79\%. Our measurements demonstrate the advantage of
using the intrinsically asynchronous and message driven programming model exposed by HPX which
enables better latency hiding, fine to medium grain parallelism, and constraint based synchronization.
HPX's uniform programming model simplifies writing highly parallel code for heterogeneous resources.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{High Performance Computing}{Runtime Parallel Systems}{N-Body}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
%\terms{Theory}
%\up
%\up
%\up
\keywords{High Performance Computing, Parallel Runtime Systems, Application Frameworks}

\newpage
\section{Introduction}

Due to the scale of today's supercomputers, users must be able to exploit multiple levels of parallelism
if they hope to achieve decent performance on today's machines.
In addition to the theoretical scaling limits described by Amdahl's Law\cite{amdahl1967validity} and Gustafson's Law\cite{gustafson88}
at least four additional factors limit application scalability, also referred to as the \B{SLOW} factors: a)~\I{\B{S}tarvation}, i.e.
current concurrent work is insufficient to maintain
high utilization of all resources, b)~\I{\B{L}atencies}, i.e. the delay intrinsic to accessing remote
resources and services deferring their responses, c)~\I{\B{O}verheads}, i.e. the
work required for the management of parallel actions and
resources on the critical execution path which is not necessary in a sequential variant, and d)~\I{\B{W}aiting for
contention resolution}, which is caused by the delays due to oversubscribed
shared resources.

% We posit that in order to achieve the goal of making even highly dynamic applications scalable,
% a new programming model is needed - a programming model which overcomes the limitations of
% how applications are written today and which makes the full parallelization capabilities of today's
% and tomorrow's heterogeneous hardware available to the application programmer in an easy and uniform way.
% This programming model has to be designed based on
% governing principles enabling a maximum of application level parallelism, while minimizing the effect of the SLOW
% factors:
% 1) The utilization of an Active Global Address Space (AGAS), without the
%       assumption of cache coherence. This implies the
%       preference of using adaptive locality management over purely static data
%       placement strategies.
% 2) The exposure of new forms of program parallelism, including fine-grained
%       parallelism %, a fundamental paradigm shift from CSP and MPI as today's
%     %prevalent programming model.
% 3) The preference for mechanisms which allow hiding latencies over methods
%       for latency avoidance.
% 4) The preference to move work to the data over moving data to the work.
% 5) The elimination of global barriers, replacing them with constraint-based
%       synchronization built on Local (and Lightweight) Control Objects (LCOs)
%       to enable the efficient use of fine-grain parallelism.
% 6) The facilitation of dynamic and heuristic resource management and
%       task-queue based scheduling utilizing information provided by runtime
%       introspection, enabling active dynamic resource management and locality
%       control for applications.
% \F{can't we cut this whole paragraph significantly? IMHO it makes broader claims than we can support by our measurements and it has little value to the reader as it's not really concerned with the n-body problem and its implementation.}

%\upp\up
We posit that in order to achieve the goal of making even highly dynamic applications scalable,
a new programming model is required. This programming model will need to overcome the limitations of
how applications are written today and make the full parallelization capabilities of today's
and tomorrow's heterogeneous hardware available to the application programmer in an simple and uniform way.
The work presented in this paper is based on HPX - a runtime system implementing such a programming model.
It is described in detail in Section~\ref{sec:hpx}. HPX is based on the set of governing principles of the ParalleX
execution model~\cite{scaling_impaired_apps,heller_dataflow,tabbal} to enable a maximum of application level parallelism, while minimizing the effect of the
\B{SLOW} factors.

%\upp\up
In order to efficiently use the proposed programming model today, already existing application frameworks need to be ported to HPX
such that those frameworks can benefit from the advanced levels of parallelism provided.
As an example, this paper describes the results from porting the parallel
simulation backend of LibGeoDecomp (see Section~\ref{sec:lgd})
to utilize HPX (see Section~\ref{sec:implementation}). Due to the highly modular
and careful design of LibGeoDecomp we were able to develop
the backend such that the user's simulation code doesn't need to be changed. Nevertheless, the new parallelism provided
by the HPX backend is fully utilized. Due to the uniform programming model, we are
able to present numbers that outscale and outperform the already existing MPI backend on heterogeneous
architectures by a significant margin
while maintaining the high productive programmability of LibGeoDecomp (see Section~\ref{sec:benchmarks}).

%\upp\up
This paper presents the results obtained from large scale runs on TACC's Stampede
resource\cite{stampede_tacc} using LibGeoDecomp's HPX backend, comparing those to
equivalent runs performed with LibGeoDecomp's MPI backend. In order to evaluate the
achieved performance we used a N-Body application written in LibGeoDecomp (see Section~\ref{sec:simulation}).
It highlights the capabilities of Stampede's heterogeneous architecture by fully utilizing
its Xeon Phi accelerators \cite{xeon_phi_webpage} combined with all cores of
the used host nodes.
% this sentence does not benefit the reader:
% After shortly describing HPX and LibGeoDecomp, the paper
% describes the N-Body application chosen for the benchmarking, the performed
% benchmarks themselves, and discusses the achieved results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\upp
\section{HPX: A General Purpose Paral\-lel Runtime System}
\label{sec:hpx}

HPX is a
general purpose parallel runtime system exposing a uniform programming model for applications of
any scale. It has been developed for
conventional architectures, such as SMP nodes, large Non Uniform Memory Access (NUMA) machines and
clusters, and heterogeneous systems such as using Xeon Phi accelerators. Strict adherence to Standard
C++11~\cite{cxx11_standard} and the utilization of the Boost C++
Libraries~\cite{boostcpplibraries} makes HPX both portable and highly
optimized. It is modular, feature-complete and designed for best possible
performance. HPX's design focuses on overcoming conventional limitations such as (implicit and explicit) global
barriers, poor latency hiding, static-only resource allocation, and lack of support for medium- to fine-grain
parallelism (see Figure~\ref{fig:hpxarch}).

\begin{figure}
  \includegraphics[width=0.99\linewidth]{figures/hpx_architecture}
    \upp
  \caption{\small{Architecture of the HPX runtime system. An incoming parcel (delivered over the interconnect)
is received by the parcel port and dispatched to the parcel handler. The
main task of the parcel handler is to buffer incoming parcels for the action manager. The action
manager decodes the parcel and creates an HPX-thread based on the encoded information.
All HPX-threads are managed by the thread manager, which schedules their execution on one
of the cores. Usually HPX creates one OS-thread for each available core. The thread
manager implements several scheduling policies, such as a global queue scheduler, where all
cores pull their work from a single, global queue, or a local priority scheduler, where each core
pulls its work from a separate priority queue. The latter supports work stealing for better
load balancing. Local Control Objects (LCOs) are responsible for synchronizing access to
shared resources and are tightly integrated with the thread scheduling to resume
threads whenever all preconditions for continuing execution of a thread are met.} }
\upp\upp
\label{fig:hpxarch}
\end{figure}

%\upp\up
% 3.) AGAS
\B{The Active Global Address Space (AGAS):} In HPX, AGAS currently is a set of (distributed)
services that implement a 128-bit global address space spanning all localities.
Those provide two naming layers in HPX. The primary naming service maps
128-bit unique, global identifiers (GIDs) to a tuple of meta-data that can be used
to locate an object on a particular locality. The higher-level layer maps hierarchical
symbolic names to GIDs. Unlike systems such as
X10~\cite{Charles:2005:XOA:1103845.1094852},
Chapel~\cite{Chamberlain07parallelprogrammability}, or UPC~\cite{UPCSpec}, which are based on PGAS~\cite{pgas_link}, AGAS
exposes a dynamic, adaptive address space which evolves over the lifetime of an
HPX application. When a globally named object is migrated, the AGAS mapping is updated, however its GID remains the
same. This decouples references to those objects from the locality that they are
located on.

%\upp\up
% 0.) Parcel transport layer (parcels, parcel port, parcel handler, action manager)
\B{Parcel Transport Layer:} HPX parcels are a form of active messages~\cite{Wall:1982:MAA:582153.582157} used for
communication between localities. In HPX, parcels encapsulate remote method calls. A
parcel contains the global name of an object to act on, a reference to one of
the object's methods and the arguments to call the method with. Parcels are
used to either migrate work to data by invoking a method on a remote entity, or
to bring pieces of data back to the calling locality. Currently, HPX
implements parcel communication over TCP/IP, Infiniband, shared memory (for
communication between localities running on the same physical resource), and on top
of low level MPI functionality (MPI\_Isend/MPI\_Irecv).
The MPI parcelport is used mainly for enabling a smooth transition of existing applications
and to ensure HPX can be run on any platforms with an existing MPI transport layer
implementation.
Each locality has a parcel port
which reacts to inbound messages and  asynchronously transmits outbound messages.
After a parcel port receives and de-serializes a message, it passes the parcel to a
parcel handler.
%The parcel handler is responsible for deciding how to process the parcel.
%The primary parcel handler currently used by HPX is the action manager.
If the target object of a parcel is local, then the action manager
converts the parcel into a HPX-thread, which is scheduled by the HPX
thread-manager.

%\upp\up
% 1.) HPX-threads and the thread-manager
\B{HPX-threads and their management:} The HPX thread-manager is responsible for
the creation, scheduling, execution and destruction of HPX-threads. In HPX,
threading uses an \I{M:N} or hybrid threading model. In this model, \I{N} HPX-threads
are mapped onto \I{M} kernel threads (OS-threads), usually one OS-thread per core.
This threading model
was chosen to enable fine-grained parallelization; HPX-threads can be scheduled
without a kernel call, reducing the overhead of their execution and
suspension. The thread-manager uses a work-queue based execution strategy with work stealing
similar to systems such as Cilk++~\cite{cilk++}, Threading Building Blocks (TBB~\cite{inteltbb}), or
the Parallel Patterns Library (PPL~\cite{microsoftppl}). HPX-threads are scheduled cooperatively, that is,
they are not preempted by the thread-manager. HPX-threads may voluntarily
suspend themselves when they must wait for data that they require to continue
execution, I/O operations or synchronization.
%In the current implementation, suspended HPX-threads are not migrated due to
%unacceptable implementation barriers, such as the challenges associated with
%moving register contexts across architectures and the costs of transmitting
%stacks. Instead, we employ split phase transactions, were work is chained
%via continuations, which send a parcel to a remote locality, if needed.
%HPX provides a set of control mechanisms called LCOs to
%manipulate the parallelization of HPX-threads.

%\upp\up
% 2.) LCOs
\B{Local Control Objects (LCOs):} LCOs provide a means of controlling
parallelization in HPX. Any object that may create a new HPX-thread or
reactivate a suspended HPX-thread exposes the required functionality of an LCO.
Support for event-driven HPX-thread creation, protection of shared data structures,
and organization of flow control are provided by LCOs. They are designed
to allow for HPX-threads to proceed in its execution
as far as possible without waiting for a particular blocking operation, such
as a data dependency or I/O, to finish. Some of the more prominent LCOs provided
by HPX are:

%\upp\up
\begin{easylist}
  % eager and lazy futures, promises
& \I{Futures}~\cite{future1, future2, Halstead:1985:MLC:4472.4478} represent results
  that are not yet known, possibly because they have not yet been computed. A
  future synchronizes access to the result value associated with it by suspending
  HPX-threads requesting the value if the value is not available at the time of
  the request. When the result becomes available, the future resumes all
  suspended HPX-threads waiting for the value. These semantics allow execution
  to proceed unblocked until the actual value is required for computation.
%  Futures
%  facilitate anonymous producer-consumer semantics; neither the producer of a value
%  or the consumer of the value need to be known at compile time. HPX provides
%  two types of futures. Eager futures initiate the calculation of their value
%  as soon as they are created, while lazy futures postpone the calculation of a
%  value until it is actually needed.
  % dataflow
%\upp\up
& \I{Dataflow objects}~\cite{Dennis74, DennisM98, dyn_dataflow} provide a powerful
  mechanism for managing data dependencies without the use of global barriers.
  A dataflow LCO ensures that a predefined function will be called once a set of
  values become available. The function is called passing along all of this data.
  % counting semaphores, binary semaphores (mutexes)
%\upp\up
& \I{Traditional concurrency control mechanisms} including
  various types of mutexes~\cite{mutex}, counting semaphores, spinlocks,
  condition variables and barriers are also exposed as LCOs in HPX. These
  constructs can be used to cooperatively suspend an HPX-thread while informing
  the HPX thread-manager that other HPX-threads can be scheduled on the
  OS-thread.
%  Because these synchronization primitives do not use kernel calls, they
%  are lightweight enough to enable control of HPX's fine-grained threading.
\end{easylist}

%\upp\up
LCOs are first class objects in HPX, they enable intrinsic overlapping of computation
and communication. This not only hides latencies, but also allows many phases of a
computation to overlap, exposing greater application parallelism.
%they have a system-widely unique global address.
They can be used to control parallelism across multiple localities. The mechanisms for
naming and referencing first class objects such as LCOs is provided by AGAS.

%\upp
\section{LibGeoDecomp -- An Auto-paral\-lelizing Library}
\label{sec:lgd}

The purpose of LibGeoDecomp\cite{libgeodecomp} is to simplify the
development of computer simulations. Typical challenges for such codes
are the adaptation to new hardware architectures and the scalability
on large-scale systems. Simulation models are generally developed by
domain scientists, e.g. physicists or material scientists. Their
productivity will be greatly increased if they can be relieved from
having to worry about the machine architecture.

%\upp\up
The basic abstraction within LibGeoDecomp is the simulation cell.
Cells are placed in a regular grid and updated once per timestep.
During the update they may access their neighbors from the last time
step. In other words, in LibGeoDecomp simulations are written as
iterative algorithms with spatial discretization. Examples for such
models are cellular automata or Lattice Boltzmann Methods. Other
models, such as N-body codes, which cannot be directly represented by
a regular grid are handled by wrapping the particles into boxes
according to their spatial location. The containers then form a
regular grid. This procedure works well if the particles are evenly
distributed, but efficiency is poor if pronounced hotspots are
present. Only local interactions can be represented.

%\upp\up
The library is written as a set of C++ class templates. User code
describes the behavior and the data stored in a single simulation cell.
It is inserted into the library as a template parameter. Interaction
of model and library is defined by a two-way callback interface: the
library calls a cell to update its state and the cell may call back
the library to retrieve the states of itself and its neighbors from
the last time step by means of a proxy object -- the so called
neighborhood.

%\upp\up
Within the library the objects which maintain the workflow of the
simulation are named \ctype{Simulators}. These implement various optimizations
such as multi-node and multi-core parallelization, overlapping
communication and calculation, parallel IO, etc. The library has
support for in-situ visualization and live steering (see Fig.~\ref{fig:lgdarch}).

%\upp\up
The key advantage of this approach is that user code and
parallelization are segregated. User code may benefit from
improvements of the parallelization without the need of modifications.
Single investments into the library benefit multiple applications.

% commenting out due to space constraints:
% Models with highly irregular densities can be represented by coupling
% multiple Simulator instances, each of which using different
% resolutions and representing different parts of the simulation space.

\begin{figure}
  \includegraphics[width=0.99\linewidth]{figures/lgd_architecture}
    \upp\upp
  \caption{High-level architecture of LibGeoDecomp: the \ctype{Simulator}
    controls the workflow. Data is stored within the \ctype{Grid}, whose
    elements, the \ctype{Cells}, are provided by the user. The \ctype{Cell} also
    contains information on the topology and boundary conditions
    required by the model.
%     The Initializer, which also needs to be
%     provided by the user, sets up the initial conditions.
    A \ctype{Writer} performs periodic output, e.g. by dumping the grid in VisIt's BOV
    format to disk. Conversely, a \ctype{Steerer} may modify the grid at
    runtime. This can be leveraged quickly patch errors within the
    simulation or to conduct experiments with low turnaround times.}
\upp\upp
\label{fig:lgdarch}
\end{figure}

%\upp
\section{Simulation Model}
\label{sec:simulation}

For our evaluation we chose the N-body model presented
in~\cite{colfax}. This model represents a larger class of similar
models (e.g. gravitating bodies or electrostatically charged
particles) and allows comparison of our results with previous
publications. The only modification we applied was to introduce a
cut-off ratio to the force calculation. This slightly decreases the
computational complexity (i.e. it increases the model's demand for
memory bandwidth).

%\upp\up
In essence, the model is an N-body simulation with short range
interaction, which can be described by the following equations:
\upp
\begin{eqnarray*}
  F_i &=& K \cdot C_i \sum_{j \in H_i} C_j \frac{R_j - R_i}{\left\Vert R_j - R_i\right\Vert + s} \\
  H_i &=& \left\{ j \in \mathbb{N} \,\middle|\, \left\Vert R_j - R_i\right\Vert \le D  \right\}
\end{eqnarray*}
\upp
\upp

%\upp\up
$F_i$ is the force acting on body $i$, $R_i$ is its location. The
force is defined by those particles which interact with the current
particle. As said, we do not take into account all particles, but just
those within a certain radius $D$. These particles are found in set
$H_i$. Particles beyond the cutoff are assumed to have only a
negligible influence. The direction and magnitude of the forces
further depends on the factors $C_i$ which -- depending on the
physical model -- may represent mass or charge of the given body, and
the model-dependent constant $K$. The parameter $s$ is often referred
to as a \emph{softening factor} and has the purpose to avoid a
division by zero if the interaction of the particle with itself is
being calculated -- or if two particles should accidentally occupy the
same position.

%\upp\up
Crucial to an efficient implementation of this model are two aspects:
the calculations need to be vectorized and the calculation of the sets
$H_i$ should not incur any overhead. The latter can be achieved by
placing all particles in parallelepipeds of size $D$, as shown in Figure~\ref{fig:nbodymodel}. These serve as
containers. Each particle can only interact with particles from its
own container or from those surrounding it. This is a
standard technique and has been used in other scalable implementations
before (e.g.~\cite{blueMatter}). As the particles move through space,
they may need to switch containers. Checking for such transitions may
be a time consuming test, but by choosing the container size slightly
too large we can defer such tests, thereby rendering the overhead
caused by this transition negligible.

%\upp\up
Vectorization requires that particles are not stored as distinct
objects, but rather in a \emph{Struct of Arrays}~\cite{zeroOverhead}
fashion. This means that each member variable (e.g. the three scalars
that make up its position) is stored as a vector for all particles
within a container. Thanks to the softening factor, our model does not
need to avoid self-interaction of the particles. This simplifies the
vectorization of the loop. In fact, the pseudocode below can be
vectorized in two ways: we can either traverse the particles in
\cvar{cell.particles} in the innermost loop in a vectorized fashion
or traverse the cells \cvar{particles} in a scalar way
and compute the interactions with multiple \cvar{p\_1} from
\ctype{this->particles}.
In the first case we would compute all acting forces for exactly one
particle \cvar{p\_1} and all particles \cvar{p\_2} from \cvar{cell}.
 The latter strategy requires the architecture
to perform a scalar load (for retrieving \cvar{p\_2}) and to broadcast
that scalar value to a vector register in an efficient fashion.
\vspace*{2em}

\begin{figure}
  \centering

  \includegraphics[scale=0.8]{figures/nbody_model}

    \upp
  \caption{Illustration of simulation model. Particles are placed in a
    container according to their physical location. All particles of
    the shaded cell may interact with the particles of their own cell
    and those cells neighboring it.}
  \upp\upp
\upp
  \label{fig:nbodymodel}
\end{figure}

\begin{lstlisting}
void addForce(Particle& p_1, const Particle& p_2)
{
  vec3 delta = p_2.pos - p_1.pos;
  p_1.force += p_2.c * delta / (norm(delta) + s);
}

for(Particle p_1: this->particles) {
  for(Cell cell: neighborhood) {
    for(Particle p_2: cell.particles) {
      add_force(p_1, p_2);
    }
  }
}
\end{lstlisting}

%\upp
\section{Implementation}
\label{sec:implementation}

% This section details how the HpxSimulator within LibGeoDecomp takes
% advantage of HPX and how this helps to coordinate the Xeon Phi and
% host CPUs on Stampede.

This section details the port of LibGeoDecomp to HPX and the implemented mechanisms
that allow the efficient use of heterogeneous resources such as the Stampede supercomputer.
As described in Section~\ref{sec:lgd}, LibGeoDecomp consists of several high-level components.
The HPX backend to LibGeoDecomp merely touches the Simulator module without the need to change
the already existing API and semantics of existing backends, such as the MPI backend. As a
matter of fact, the HPX backend is closely modeled after the MPI implementation to allow the reuse
of the already existing infrastructure for domain decomposition and synchronization.

\subsection{Overview}
\label{sec:implementation:overview}
The simulator features a layered design, which
separates the domain decomposition from the synchronization of the
node domains and the intra-node threading. The design needs to satisfy
two goals: reduce the impact of network latency and equalize the load
on CPUs and accelerators. Latency can be hidden by overlapping
communication and calculation. This in turn requires a low-over\-head
communication infrastructure and the ability to make asynchronous
progress. Load equalization mandates a tunable domain decomposition
scheme. In LibGeoDecomp a domain decomposition (or partition) is a
function which maps the nodes to sets of coordinates. These coordinate
sets represent the domains of the nodes. Their sizes can be tuned via
a weight vector. The design can be summarized by the following modules: \\
\B{\ctype{Simulator}:} The \ctype{Simulator} acts as the main interface to the
user. It acts as the glue code to set up all the following classes. It will set up as many
\ctype{UpdateGroups} as requested at runtime. \\
\B{\ctype{UpdateGroup}:} An \ctype{UpdateGroup} represents the entity to create
the simulation \ctype{Stepper} and the neighbor\-hood communications
defined by the \ctype{PatchAcceptor} and \ctype{PatchProvider}. Additionally,
the partitioning is done within an \ctype{UpdateGroup} which is implemented in a
\ctype{PartitionManager}. \\
\B{\ctype{PartitionManager}:} As the discrete domain of computation needs to be
decomposed, or partitioned, in order to be parallelized efficiently, a
\ctype{PartitionManager} is needed to implement various partitioning strategies
and determine the simulation domain as well as the ghostzones of a certain
\ctype{UpdateGroup}. \\
% For a matter of seperation of concerns, this is handled by
% the \ctype{PartitionManager}. \\
\B{\ctype{Stepper}:} The \ctype{Stepper} class represents the main simulation
control flow implementation. The \ctype{Cell}'s update function as well as the
ghost zone exchange as provided by the \ctype{PatchAcceptors} and
\ctype{PatchProviders} is done here. This class is mainly responsible for the
scalability of LibGeoDecomp and is the class in which the parallelization has to
happen. A more detailed discussion of the algorithm implemented and the
parallelization strategy can be found in
Subsection~\ref{sec:implementation:parallelization}. \\
\B{\ctype{PatchAcceptor}:} A \ctype{PatchAcceptor} provides an abstraction for
the \ctype{Stepper} which is used to retrieve the state of the Grid in the
current time step. It used to either notify a \ctype{Writer} to write the grid
elements (see Section~\ref{sec:lgd}) or to update it's neighboring
\ctype{UpdateGroups} ghost zones through a \ctype{PatchLink}. The setting of a
neighboring ghost zone can be completely behind the computation through the
\ctype{Stepper}. \\
\B{\ctype{PatchProvider}:} Similar to the \ctype{PatchAcceptor}, the
\ctype{Patch- Provider} is providing an abstraction to set the portions of the Grid
at the current time\-step. It is used to either notify a \ctype{Steerer} to set a
new state of the current's grid elements (see Section~\ref{sec:lgd}) or to set
the ghost zone retrieved from a neighboring \ctype{UpdateGroup} through a
\ctype{PatchLink}. This is the only place in LibGeoDecomp's parallelization
backend where a synchronization between the different \ctype{UpdateGroups}
happens as it is guaranteed that the time\-steps match between \ctype{UpdateGroups}.

\begin{figure}
  \includegraphics[width=0.99\linewidth]{figures/lgd_detailed_arch}
    \upp
  \caption{
    Main objects and their interactions when using LibGeoDecomp's HPX
    backend. The idea is to reduce complexity by decomposing the
    codebase into specialized classes. The \ctype{Simulator} manages
    the workflow of the simulation. It will contain one
    \ctype{UpdateGroup} per NUMA. Each \ctype{UpdateGroup} is
    responsible for a sub-domain of the grid. Which domain exactly is
    determined by the \ctype{PartitionManager}. The \ctype{Stepper}
    contains the temporal and spatial loops. It will call back the
    user code within the \ctype{CELL} class. Halos are synchronized by
    the \ctype{PatchLinks}. A \ctype{Writer} is an output plugin,
    while a \ctype{Steerer} can be used for live-steering. }
\upp\upp\upp\upp
\label{fig:lgdhpxarch}
\end{figure}

%\upp\up
As noted above, both the Scalable MPI backend (implemented in the class
\ctype{HiParSimulator}) and the HPX backend (implemented in the class \ctype{HpxSimulator})
make use of this generic structure while maintaining as much API compatibility as
possible. The only notable difference in the interface lies within the creation
of an \ctype{Simulator} object. Where with the \ctype{HiParSimulator}, the number of
\ctype{UpdateGroups} are equal to the number MPI Ranks created, the \ctype{HPXSimulator} allows the user to decide
how many \ctype{UpdateGroups} are created per node by inputting an additional parameter. The
details and benefits to this approach are discussed in the following subsection.

\subsection{Parallelization and scalability considerations}
\label{sec:implementation:parallelization}

As discussed previously, the heavy lifting of the parallelization efforts of
LibGeoDecomp lie within the responsibilities of the \\ \ctype{UpdateGroup},
\ctype{Stepper}, and \ctype{PatchLink}. \\
An \ctype{UpdateGroup} creates a partition for itself based on the selected
partitioning scheme, which not only determines which partition belongs to the
current \ctype{UpdateGroup} in question, but also determined which \ctype{UpdateGroups} have
corresponding neighboring regions. The size of a partition is determined by an
initial weight vector, which might consist of equal weights for homogeneous runs,
or consist of different weights, for heterogeneous runs. This weight factor is
determined by the user and defined in the \ctype{Cell} implementation, which takes
different computation speeds of the various processing units involved in the
simulation into account. \\
In addition, two \ctype{PatchLinks} are created for each corresponding neighboring Region;
% for each corresponding neighboring Region, two \ctype{PatchLinks} are
% created which
They are responsible for (a) receiving a ghostzone fragment (handled by a
\ctype{PatchProvider}) or (b) sending a ghostzone fragment (handled by a
\ctype{PatchAcceptor}). As described in the previous subsection, the \ctype{Patchlink}
of a \ctype{Patch\-Provider}, is the only place where a single \ctype{UpdateGroup}
is synchronized with its neighbors. It is important to note that no collective
operations are used here; therefore, the implementation is scalable by design. The
\ctype{HpxSimulator} and \ctype{HiParSimulator} share this important design
decision, however, the implementation specifics are worthy to note as they highlight
the advantages of the HPX programming model over MPI. While within the
\ctype{HiPar\-Simulator} one \ctype{UpdateGroup} per rank is created and the
communication within the \ctype{PatchLink} is implemented via the two-sided MPI asynchronous communication primitives, the HPX Backend is
able to leverage the AGAS (see Sec.~\ref{sec:hpx}).
% and therefor creating a unified
% programming interface for the ghost zone exchange for distributed and shared memory
% synchronization of the \ctype{PatchLinks}.
It creates a varying number of \ctype{UpdateGroup}
components per node based on the compute requirements of a specific node.
Additionally, the \ctype{Patch\-Links} do not need to communicate via low level
primitives such as \cfunc{MPI\_Isend} and \cfunc{MPI\_Irecv} but can rely on
invoking a (possibly remote) set function of the neighboring \ctype{UpdateGroup}
component taking full advantage of the unified program model provided by HPX. This
mechanism is not only implemented in a truly Object Oriented fashion, but it is also inherently
asynchronous. By moving the \ctype{UpdateGroups} into the AGAS, and thereby only needing one HPX
locality per node, we gain considerable advantages over the MPI programming model which requires
one MPI process per CPU core. This avoids, possibly expensive, inter-process communications which leads to an
increased scalability on a single node.

%\upp\up
The techniques described above amount to a complete port of LibGeoDecomp to
HPX. However, only a small portion of the HPX parallel runtime system is used.
To fully exploit the potentials of the emerging technology, the \ctype{Stepper}
class will need to be ported to HPX as well. However, for now, the \ctype{Stepper} used by
MPI backend can be used without any further modifications. The
conventional \ctype{VanillaStepper} is outlined in Fig.~\ref{alg:vanilla_stepper}:
For each timestep, the inner region is updated, once done, we notify the
\ctype{PatchAccepters} in order to retrieve the new ghostzones. Afterwards, our
inner ghostzone can be updated and, once finished, sent to the neighboring
\ctype{UpdateGroups}.

\begin{figure}
    \begin{lstlisting}
for(Region r: innerRegion) {
  update(r, oldGrid, newGrid, step);
}
swap(oldGrid, newGrid);
++step;
for(Region r: outerGhostZoneRegion) {
  notifyPatchProviders(r, oldGrid);
}

for(Region r: outerGhostZoneRegion) {
  update(r, oldGrid, newGrid, step);
}
for(Region r: innerGhostZoneRegion) {
  notifyPatchAccepters(r, oldGrid);
}
    \end{lstlisting}
    \upp\upp
    \caption{\ctype{VanillaStepper}: Algorithm sketch of a Stepper for
    LibGeoDecomp. This is a basic outline for how the simulation stepping inside
    LibGeoDecomp works. The implementation is fully serial.}
    \label{alg:vanilla_stepper}
    \upp
    \upp
    \upp
    \up
\end{figure}

%\upp\up
Due to the serial nature of a single MPI process, the outlined algorithm is as
good as it can get. However, the advanced parallelization techniques provided by
HPX open the doors to further improve and take full advantage of the parallel
capabilities of a single CPU. The described code can be fully futurized.
Futurization is a technique which allows users to turn otherwise serial code into a
chain of asynchronously executed functions. The serial control flow is
transformed into a sequence of depending continuations to previous calculations.
A simple loop without dependencies can be simply formulated as a loop where every
loop body is executed in parallel. A possibly depending calculation can simply be
chained by passing a continuation function which will be executed whenever every
chunk of the loop has finished (as described in~\cite{cxx11_n3634} and \cite{cxx11_n3722}).
This will lead us to the futurized version of the \ctype{VanillaStepper}, the
\ctype{HpxStepper} (see Fig.~\ref{alg:futurized_stepper}). The algorithm works in
the same way as the one presented in Fig.~\ref{alg:vanilla_stepper}. The
distinctions between the two codes are that different independent regions are computed and
notification of the \ctype{PatchAccepters} and \ctype{PatchProviders} are performed in parallel.  In
addition we break up each step into a sequence of continuations. This results in taking
a very coarse grained function and reducing it into multiple fine grained functions
whose parts are executed in parallel.
\begin{figure}[ht]
    \begin{lstlisting}
typedef vector<hpx::future<void>> future_vector;
future_vector updateFutures;
for(Region r: innerRegion) {
  updateFutures.push_back(
    hpx::async(update, r, oldGrid, newGrid, step));
}
hpx::when_all(updateFutures,
[](const future_vector&) {
  swap(oldGrid, newGrid);
  ++step;
  future_vector notifyFutures;
  for(Region r: outerGhostZoneRegion) {
    notifyFutures.push_back(
      hpx::async(notifyPatchProviders, r, oldGrid));
  }
  hpx::when_all(notifyFutures,
  [](const future_vector&) {
    future_vector updateFutures;
    for(Region r: outerGhostZoneRegion) {
      updateFutures.push_back(
        hpx::async(update, r, oldGrid, newGrid, step));
    }
    hpx::when_all(updateFutures,
    [](const future_vector&) {
      future_vector notifyFutures;
      for(Region r: innerGhostZoneRegion) {
        notifyFutures.push_back(
          hpx::async(notifyPatchAccepters, r, oldGrid));
      }
    });
  });
});
    \end{lstlisting}
    \upp\upp
    \caption{Futurized \ctype{VanillaStepper}, the \ctype{HpxStepper}. Due to the
    Advanced parallelization strategies provided by HPX, this Stepper implementation
    is now fully parallel and scalable due to it's highly asynchronous nature.}
    \label{alg:futurized_stepper}
    \upp
    \upp
    \upp
    \upp
\end{figure}

%\upp\up
By applying the described techniques, we gain a powerful backend which is able to
make use of all parallel resources. On the node level, we benefit from futurization
and the ability to have only one process per node. When running the application in
distributed, we are profiting from the unified programming model which gives us
increased asynchronity which in turn leads to better latency hiding by being able
to properly hide communications behind useful computation. All this was achieved
while being 100\% API compatible for existing LibGeoDecomp applications, which means they
can immediately benefit from the HPX backend.

%\upp
\section{Benchmarks}
\label{sec:benchmarks}

In order to evaluate our developed approaches, and test the scalability of our
the newly developed library we used the simulation model as described in
Section~\ref{sec:simulation}. The computing resource used is TACC's
Stampede~\cite{stampede_tacc}. It consist of a total of 6400 nodes with two Intel Xeon
E5 processors and one Intel Xeon Phi coprocessor (see Table~\ref{tab:stampede}).
The compute nodes are interconnected with Mellanox FDR InfiniBand technology
(56 Gb/s) in a 2-level fat-tree topology. The complete system is a 10 PFLOPS
cluster. In our weak scaling experiments, we scale the problem size with the number of cores.
For each core we assigned 1000 grid elements. The distributed HPX
applications use the MPI parcelport since it currently provides the best
performance.

\begin{table}
    \centering
    \vspace*{0.2em}
    \begin{tabular}{lcc}
      \toprule
                                  & Intel Xeon E5           & Intel Xeon Phi \\
      \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(l){3-3}
        Clock Frequency           & 2.7 (3.5 Turbo) GHz & 1.1 GHz        \\
%      \hline
        Number of Cores           & 16 (2x8)                & 61             \\
 %       \hline
        SMT                       & 2-way (deactivated)     & 4-way          \\
  %      \hline
        NUMA Domains              & 2                       & 1              \\
  %      \hline
        RAM          & 32 GB                   & 8 GB           \\
  %      \hline
        SIMD                      & AVX (256 bit)           & MIC (512 bit)  \\
  %      \hline
        GFLOPS & 691.2 (896.0 Turbo)     & 2147.2         \\
 %       \hline
        Microarchitecture         & Sandy Bridge            & Knights Corner \\
        \bottomrule
    \end{tabular}
    \upp\upp
    \caption{Overview of the processors built into one compute node of the
    Stampede supercomputer. GLFOPS are presented in single precision}
    \label{tab:stampede}
\end{table}

%\upp\up
As a first measure we conduct weak scaling tests on a single node. The theoretical
peak performance of the two Xeon E5 processors is 691.2 GFLOPS, or 896
GFLOPS with the Intel Turbo Boost technology. This performance can be reached by
simultaneously scheduling 8 multiply and 8 addition operations
per cycle. Our computational kernel introduced in Section~\ref{sec:simulation}
has a ratio of multiply/add to other floating instructions of 4:6, as such the
theoretical peak performance of our algorithm is 552.9 GFLOPS on the Xeon E5
processor (assuming the CPU is almost always running in its turbo mode).

\begin{figure}[ht]
  \includegraphics[width=0.99\linewidth]{figures/times_host_smp}
    \upp
  \caption{Weak scaling execution times for the HPX and MPI N-Body codes collected on a
    single SMP node while varying the number of cores used from 1 to 16. The figure shows
    the times for the overall \I{Sim} and the overheads introduced by \I{Comm}.
    Additionally demonstrating the benefit of the futurization technique, as no extra
    communication is needed for the HPX backend while the MPI backend needs to
    perform extra actions for the halo exchange. By that, the HPX backend only
    has 8\% parallel overhead, while the MPI backend shows $\sim$27\% overhead. Which
    accounts for a sustained performance of $\sim$546 GFLOPS using the HPX backend.}
%     the consequent overlapping computation with communication
%     enabled by HPX which allows to significantly reduce the effective overheads imposed by the
%     communication step.}
\label{fig:times_host_smp}
     \upp
     \upp
     \upp
\end{figure}

%\upp\up
Fig.~\ref{fig:times_host_smp} shows the results obtained from running on a single
node. While the MPI backend needs to perform inter-process communication in order
to exchange the ghostzones, the HPX is able to overcome this limitation by
efficiently parallelizing the simulation steps (see Section~\ref{sec:implementation:parallelization}).
We are able to reach $\sim$98\% peak performance with the \ctype{HpxSimulator} while
MPI backend is only able to achieve $\sim$63\%. The sub-optimal performance of the MPI
backend is mainly due to the extra overheads introduced by inter-process communication
and the need to update the ghostzones separately. Even though one would think
that using high level parallelization APIs, like the C++ API exposed by HPX and
high level application frameworks like LibGeoDecomp, would create a significant overhead,
the benchmark shows that they impose an overhead of only $\sim$8\%.

\begin{figure}[ht]
  \includegraphics[width=0.99\linewidth]{figures/scaling_data_phi}
    \upp\upp
  \caption{Weak scaling results for the HPX N-Body code collected on a
      single Xeon Phi coprocessor while varying both the number of cores used and
      the number of threads per core. The figure shows excellent weak scaling
      behavior of the HPX backend. The overall performance doesn't increase
      much more after two threads per core are used. We are able to sustain a peak
      performance of 1504.7 GFLOPS using all 244 available hardware threads which
      is equivalent to a parallel overhead of $\sim$11\% and $\sim$89\% of the
      theoretical achievable peak performance.}
\label{fig:scaling_data_phi}
\upp
\end{figure}

%\upp\up
In addition to the scaling experiments on the host CPU, we are interested in how
well our code works on the Intel Xeon Phi architecture. Due to the lack of OpenMP
support in LibGeoDecomp, we only ran experiments with the HPX backend. To use the
Xeon Phi, we compiled native binaries for the coprocessor. Similar to the host
CPU, the Xeon Phi coprocessor is able to schedule one, so called, fused multiply
add (fma) instruction, which is able to perform 16 single precision floating
point operations in a single cycle. As such, our algorithm has a theoretical peak
of 1717.7 GLFOPS.
%\begin{figure}
%  \includegraphics[width=0.99\linewidth]{figures/times_phi}
%  \caption{\F{insert caption}}
%\label{fig:times_phi}
%\end{figure}
We are able to sustain the same performance characteristics we see on the host
processor when running on the Xeon Phi (see Fig.~\ref{fig:scaling_data_phi}). The
futurization approach is able to scale even on this architecture. However, due to
the architectural specifics of its in-order nature, the usage of more than 2 cores
per thread only leads to a minimal advantage. Overall the HPX backend is
able to achieve 89\% of the peak performance on a single Xeon Phi.
Achieving an overhead of only 11\% is remarkable as no
further modifications to the runtime system or the application framework
were necessary besides the intrinsics that were used in the computational kernel.

%\upp\up
To further test the scalability of our approach, we ran our benchmark on multiple
nodes. We were able to scale up to 1024 nodes, using 16384 Xeon E5 cores.
Furthermore, heterogeneous runs using up to 16 nodes utilizing both, the Xeon E5 host CPUs and one Xeon Phi
coprocessor on each node, using a total of 4160 hardware threads, have been conducted. For the
scaling experiment without the coprocessor, we compared the HPX backend with the
MPI backend. Our results show that both backends are able to scale well to up to
1024 nodes (see Fig.~\ref{fig:results_hosts}), reaching a parallel efficiency of
$\sim$80\% (HPX) and $\sim$66\% (MPI). The HPX backend is able to outperform the
MPI backend at scale by $\sim$8\% and is able to reach a sustained performance of
$\sim$0.35 PFLOPS. The main reason for this performance gain can be attributed to
the inherently asynchronous nature of the HPX runtime system which leads to
better latency hiding at scale (see Fig.~\ref{fig:times_hosts}) which allows
efficient overlapping of computation and communication and furthermore reduces
the overheads introduced by communication.
\begin{figure}[ht]
  \includegraphics[width=0.99\linewidth]{figures/scaling_data_hosts}
    \upp\upp
  \caption{Weak scaling performance results for the HPX and MPI N-Body codes collected for runs
    on the host's cores only (16 cores per node) while increasing the number of nodes from
    1 to 1024 (16 to 16384 cores). On 1024 nodes, the HPX code outperforms the equivalent
    MPI code by $\sim$8\% and reaches a performance of $\sim$0.35 PFLOPS.}
\label{fig:results_hosts}
\upp\upp
\end{figure}
\begin{figure}[ht]
  \includegraphics[width=0.99\linewidth]{figures/times_hosts}
    \upp\upp
  \caption{Weak scaling execution times for the HPX and MPI N-Body codes collected on
    runs for host's cores only (16 cores per node) while increasing the number of
    nodes from 1 to 1024). The figure shows the times for the overall \I{Sim}
    and the overheads introduced by \I{Comm}. It can be seen that both
    backend implementations are able to consequently overlap computation with
    computation, with a slight advantage to the HPX backend at scale which is
    caused by the advanced asynchronous capabilities of the runtime system.}
  \label{fig:times_hosts}
\upp\upp\up
\end{figure}
The symmetric benchmark runs (using both, the host processors and the coprocessor
of a node) were only conducted with the HPX backend for the same reason as for the
single Xeon Phi runs. Additionally, we needed to take the different speeds of the
now heterogeneous processors into account. As the Xeon Phi is about 2.75 times
faster then the Xeon E5 processors, we chose the partitioning in such a way, that
the Xeon Phi gets 2.75 times more grid elements.

\begin{figure}[ht]
\vspace*{2em}
  \includegraphics[width=0.99\linewidth]{figures/scaling_data_hosts_phis}
    \upp\upp
  \caption{Weak scaling performance results for the HPX N-Body code collected for symmetric runs
     on the host's cores (16 cores per node) combined with all cores of the
    associated Xeon Phi accelerators (61 cores, 4 hardware threads each)
    while increasing the number of hosts from 1 to 16. On 16 nodes, the
    HPX application reaches a performance of $\sim$19.5 TFLOPS.}
\label{fig:results_hosts_phis}
\end{figure}

%\upp\up
The results (see Fig.~\ref{fig:results_hosts_phis}) show a sustained performance
of $\sim$19.5 TFLOPS when using a total of 4160 hardware threads (256 Xeon E5 cores
and 976 Xeon Phi cores, 4 hardware threads each). The parallel efficiency is $\sim$73\% at the achieved
scale. The reduced efficiency in comparison to the runs on the hosts only can be
explained by an additional overhead introduced by communicating
with the coprocessor; the communication always requires an extra hop over the PCI
Express bus of the host system. In addition, we were not able to scale beyond 16
nodes at this time as there are still major problems with the underlying MPI software
stack provided by Intel which could not be solved in time.

% Commenting out this figure ... don't think there is any additional value here
% and we need to save some space :(
% \begin{figure}
%   \includegraphics[width=0.99\linewidth]{figures/times_hosts_phis}
%   \caption{\F{insert caption}}
% \label{fig:times_hosts_phis}
% \end{figure}

% I don't think we should include the MPI benchmarks on the Phi ... let's see if
% we can argue good enough to need them.
% \begin{figure}
%   \includegraphics[width=0.99\linewidth]{figures/times_phi_mpi}
%   \caption{\F{insert caption}}
% \label{fig:times_phi_mpi}
% \end{figure}


\upp
\section{Conclusion}

The work presented in this paper shows how the unified programming model of the
HPX runtime system can be efficiently used to implement C++ application
frameworks. We successfully ported the parallelization backend of LibGeoDecomp to
use HPX. We used a three dimensional N-Body Simulation written in LibGeoDecomp
to compare the HPX backend with an already existing MPI backend. We are able to
show perfect scaling at a single node level by reaching $\sim$98\% peak
performance of a single node of the Stampede supercomputer. In addition, due to
advanced parallelization techniques, we were able to show $\sim$89\%
peak performance when using the many-core Xeon Phi coprocessor. Furthermore, the
scalability of the HPX backend could be proofed by scaling the code to up to 1024
nodes using only the host CPUs of the Stampede supercomputer (overall 16384 cores) by reaching a
parallel efficiency of $\sim$79\% and a sustained performance of $\sim$0.35
PFLOPS, outperforming and outscaling the MPI backend by $\sim$8\%. Running this code on up to
16 nodes while utilizing the host and the coprocessor, the HPX backend was able to sustain
a performance of $\sim$19.5 TFLOPS while reaching a parallel efficiency of $\sim$73\%.

%\upp\up
Our results show that HPX can be efficiently used for homogeneous large scale applications
as well as scaling in heterogeneous environments. However, to fully utilize future and
current Peta-FLOP scale supercomputers, we need to advance further. The ability
to make use of migration within AGAS has to be refined in order to dynamically balance
load, as we were facing limitations with static load balancing in our heterogeneous
benchmarks. Additionally, migration will make it easier to write work-imbalanced applications and improve
overall fault tolerance.

%\upp
% use section* for acknowledgement
\section{Acknowledgments}

This work was supported by the Bavarian Research Foundation (Bayerische
Forschungsstfitung) funding the project "ZERPA - Zerst\"orungsfreie
Pr\"ufung auf heterogenen Parallelrechner- Architekturen (AZ-987-11)",
the XSEDE allocation ASC130032, and by the NSF award 1240655.

\small
\bibliographystyle{abbrv}
\bibliography{bibliography,pxBib}

\end{document}
