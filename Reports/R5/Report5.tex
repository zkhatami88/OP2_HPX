\documentclass[conference]{IEEEtran}
%\documentclass{sig-alternate-05-2015}
%\documentclass{article}
%\documentclass[10pt]{IEEEtran}
%\usepackage[cmex10]{amsmath}
%\usepackage[tight,footnotesize]{subFigure}
\usepackage{mdwmath}
%\bibliographystyle{IEEEtran}
%\bibliographystyle{acm}
\usepackage{mdwtab}
\usepackage{cite}
%\usepackage[dvipdfmx]{graphicx}
\usepackage{graphicx}
%\usepackage[numbers]{natbib}
\usepackage{notoccite}
%\usepackage[natbib=true,style=numeric,sorting=none]{biblatex}
\usepackage[...]{xcolor}
%\addbibresource{References}
\usepackage{listings}
\lstloadlanguages{C++,Pascal}

\usepackage[ampersand]{easylist}
\usepackage{times}
%\usepackage{mathtime}
\usepackage{booktabs}


\usepackage{color}
\usepackage[ampersand]{easylist}
\usepackage{times}
%\usepackage{mathtime}
\usepackage{booktabs}
\usepackage{listings}
\lstloadlanguages{C++,Pascal}

\usepackage{bmpsize}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{singlelinecheck=on}
\usepackage{multicol}
\usepackage{fancyhdr}
\pagenumbering{gobble}
\usepackage{lipsum}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage[export]{adjustbox}
%\usepackage{amsmath}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{mdframed}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\usepackage{array}
%\usepackage{hyperref}
%\usepackage{geometry}
\usepackage[11pt]{moresize}


\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{N}{@{}m{0pt}@{}}

\usepackage{balance}
\usepackage[export]{adjustbox}
\usepackage{epstopdf}
\usepackage{fixme}
\fxusetheme{color}
\fxsetup{
     status=draft,
     author=,
     layout=inline, % also try footnote or pdfnote
     theme=color
}


% Settings for the lstlistings environment
\lstset{
language=C++,                       % choose the language of the code
basicstyle=\footnotesize\ttfamily,  % the size of the fonts that are used for the
                                    % code
numbers=none,                       % where to put the line-numbers
numberstyle=\tiny,                  % the size of the fonts that are used for the
                                    % line-numbers
stepnumber=1,                       % the step between two line-numbers. If it's
                                    % 1 each line will be numbered
numbersep=5pt,                      % how far the line-numbers are from the code
%backgroundcolor=\color{gray},      % choose the background color. You must add
                                    % \usepackage{color}
showspaces=false,                   % show spaces adding particular underscores
showstringspaces=false,             % underline spaces within strings
showtabs=false,                     % show tabs within strings adding particular
                                    % underscores
keywordstyle=\bfseries\color{blue},  % color of the keywords
commentstyle=\color{purple},     % color of the comments
stringstyle=\color{red},        % color of strings
captionpos=b,                       % sets the caption-position to top
tabsize=2,                          % sets default tabsize to 2 spaces
frame=tb,                       % adds a frame around the code
breaklines=true,                    % sets automatic line breaking
breakatwhitespace=false,            % sets if automatic breaks should only happen
                                    % at whitespace
escapechar=\%,                      % toggles between regular LaTeX and listing
belowskip=0.3cm,                    % vspace after listing
morecomment=[s][\bfseries\color{blue}]{struct}{\ },
morecomment=[s][\bfseries\color{blue}]{class}{\ },
morecomment=[s][\bfseries\color{blue}]{public:}{\ },
morecomment=[s][\bfseries\color{blue}]{public}{\ },
morecomment=[s][\bfseries\color{blue}]{protected:}{\ },
morecomment=[s][\bfseries\color{blue}]{private:}{\ },
morecomment=[s][\bfseries\color{black}]{operator+}{\ },
xleftmargin=0.1cm,
%xrightmargin=0.1cm,
}

\definecolor{fxnote}{rgb}{0.8000,0.0000,0.0000}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\F}[1]{\B{\textcolor{red}{FIXME: #1}}}

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

   
\begin{document}


%\setcopyright{acmcopyright}

%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{SPAA'28}{July 11--13, 2016, Asilomar State Beach, CL, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}

\title{OP2 Optimization with HPX for Improving Parallel Applications Scalability}
\author{Zahra Khatami$^{1,2}$, Hartmut Kaiser$^{1,2}$ and J. Ramanujam$^{1}$ \\$^{1}$Center for Computation and Technology, Louisiana State University\\$^{2}$ The STE$||$AR Group, http://stellar-group.org
}
   
\maketitle

\begin{abstract}

Computer scientists and programmers face the difficultly in improving application scalability while using the conventional techniques. So, advanced runtime system is required to take full advantage of the available parallel resources in order to achieve to the highest parallelism level as possible. In this paper we present the capability of HPX in achieving a desired scalability in parallel applications compared with the other programming models. 

OP2 is an active library, which provides a framework of the parallel execution for unstructured grid applications on different multi-core/many-core hardware.  OpenMP is used for a loop parallelization within an application code generated with OP2 for both single-threaded and multi-threaded machines. We use HPX for parallelizing an application generated with OP2 instead of using OpenMP. Our results shows the advantage of using asynchronous programming model implemented by HPX, which enables the better scalability due to the better latency hiding and using a fine-grain parallelism.

We compare the performance results of using HPX and OpenMP for a loop parallelization within an Airfoil an Jac applications available as an example of OP2 library. The results shows an improvement in OP2 parallelization performance while using HPX. We present the results of a strong scalability test for Airfoil application on one node with up to 64 threads. We observe on an average by about 35\% scaling improvement in using \texttt{hpx::parallel::for\_each} and 12\% scaling improvement in using \texttt{hpx::async} of OP2 performance for an application parallelization compared with OpenMP.

\end{abstract}


\section{Introduction}
Nowadays, a parallel application scalability is one of the major challenges when using the conventional techniques \cite{r11,r12}. To achieve an efficient scalability, it is needed to take full advantage of all available parallel resources. Avoiding resource starvation and hiding overheads can overcome this challenge and significantly improve a parallelism level. Parallelization is implemented with decomposing the domain space into several sub-domains and assigning each of them to the group of the processors. However, the overhead time due to the communication between processors inhibits the desired scalability. As a results, in addition to space, time should be considered as an another factor which helps to get a maximum possible parallelism level\cite{r7} \cite{r14}. So the parallelization should be done in both space and time domains. 

HPX \cite{r19} overcomes this difficulty by significantly reducing the SLOW factors \cite{r6}. SLOW factors avoid the parallel scalability, which are explained as: Starvation, which is due to the poor utilization of resources; Latencies, which is the related delay for accessing to the remote resources; Overhead, which is the cost for managing parallel actions; Waiting, which is the related delay due to the shared resources \cite{r7}. 

HPX is a parallel C++ runtime system that aims to use the full parallelization capabilities of today's and tomorrow's hardware available for an application. HPX implements the concepts of the ParalleX execution model \cite{o9,o10,o11} on conventional systems including Windows, Macintosh, Linux clusters, XeonPhi, Bluegene/Q and the Android. 

 HPX enables asynchronous task execution which results in having a parallelization in both space and time domains. As a result, it removes a global barrier synchronization and improves a parallel performance. In this paper, HPX is used to improve a performance of OP2 for an application scalability.

Op2 provides a framework of the parallel execution for unstructured grid applications \cite{o1}. OP2's design and development is presented in \cite{o2,o12}. With Op2, applications can be targeted to execute on different multi-core/many-core hardware \cite{o2,o3}. To achieve further performance gains with OP2 on modern multi-core/many-core hardware, some optimizations should be applied to improve performance for different parallel applications. This can be obtained by avoiding the SLOW factors as much as possible. OpenMP is used for a loop parallelization in OP2 on a single node and also on the distributed nodes using MPI. \texttt{\#pragma omp parallel for} used for a loop parallelization in OP2 causes an implicit global barrier at the end of loop, which does not result in an efficient speedup. Using HPX parallelization methods instead of OpenMP helps eliminating these SLOW factors.

This paper describes the results from porting the parallel simulation backend of OP2 to utilize HPX. The performance of OP2 optimization with HPX is studied on a multi-core and many-core platforms through a standard unstructured mesh finite volume computational fluid dynamics (CFD) application, called Airfoil, which uses OP2 API and it is written in C. The experimental results are studied about an application scalability while using HPX in OP2 for a loop parallelization and these results are compared with OpenMP performance.

The remainder of this paper is structured as following: Section \ref{sec:op2} explains briefly about OP2. An overview of the HPX is presented in Section \ref{sec:hpx} with the key features that distinguish it from the conventional techniques. Section \ref{sec:air} presents the Airfoil application used in this research and gives details of using HPX for loop parallelization used in OP2. The experimental tests and the strong scaling are presented in Section \ref{sec:res}. Section \ref{sec:future} discusses about the future works for OP2 optimization.

\section{OP2}
\label{sec:op2}

This paper presents an optimization study of the OP2 ``active" library \cite{o1}. OP2 utilizes the source-to-source translation which targets a single application code to be written for different backend hardware platforms \cite{o2,o3,o4}. OP2 generates a code for single-threaded on a CPUs, multi-threaded using OpenMP for single SMP node of multi-core CPUs, using CUDA for a single NVIDIA GPU, using MPI and OpenMP for a cluster of CPUs and using MPI and CUDA for a cluster of NVIDIA GPUs \cite{o4}. 

OP2 API has been developed for an unstructured grids. So the algorithm includes four different parts: sets, data on sets, mapping connectivity between the sets and computation on the sets \cite{o2,o7}. Sets can be nodes, edges, faces or other elements. Data are values and parameters associated with these sets. Map is used to define a connectivity between sets. There is two different kinds of $loops$ defined in OP2: $indirect~loop$ and $direct~loop$. The operation over a set is done within a loop in an application code. Arguments passed to loops in OP2 have an explicit indication that how each of them are accessed within a loop : \texttt{OP\_READ} (read only), \texttt{OP\_WRITE} (write) or \texttt{OP\_INC} (increment - to avoid race conditions due to indirectly accessed) \cite{o1}. If data is accessed through a mapping, the loop is an $indirect$ loop. Otherwise, it is a $direct$ loop. 


OP2's design is based on achieving a near-optimal performance for scaling on multi-core processors. In \cite{o3,o4}, it is studied that OP2 API is able to produce a near-optimal performance in a parallel loops for different frameworks without the intervention of the application programmer. However, processors starvation, latencies and overhead communications in parallelization using conventional techniques usually inhibits a desired scalability. Most of the parallelization methods are based on the fork-join model. In the fork-join model, the computational process will be stopped if the results from the previous step are not completed yet. As a result, there is always a global barrier after each step. 

\texttt{\#pragma omp parallel for} is used in a code generated with OP2 for a single-threaded and also for a multi-threaded machine. There is an implicit global barrier using \texttt{\#pragma omp parallel for}, which avoids extracting an optimal parallelism from a parallel application. In this research, HPX is used for loop parallelization for a single-threaded and a multi-threaded machine instead of using OpenMP. The performance of HPX is explained in more details in section \ref{sec:hpx}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            


In this research Airfoil application is studied that is presented in \cite{o8}. This application includes both $direct$ and $indirect$ loops that all of them are parallelized with OpenMP. Optimization of Airfoil application with HPX for parallelizing loops used in a code generated with OP2 is discussed in more details in section \ref{sec:air}.                                                                                                                                                                                                                                      

The source-to-source code translator for OP2 is written in Matlab and Python \cite{o3}. Python source-to-source code translator is used in this research and some part of it is modified to automatically generate the HPX commands instead of OpenMP within an application. 


\section{HPX}
\label{sec:hpx}

In order to hide latencies even for very short operations, having a light-weight threads is needed which should have the extremely short context switching times, that results in becoming optimally executable within one cycle. HPX is a parallel C++ runtime system that facilitates distributed operations and enables fine-grained task parallelism. The fine-grained tasks result in better load balancing and lower communication overheads. HPX has been developed  to overcome conventional limitations such as global barriers and poor latency hiding \cite{r6} by embracing a new way of coordinating parallel execution. It has been developed for different architectures, such as large Non Uniform Memory Access (NUMA) machines, SMP nodes and systems using Xeon Phi accelerators.


HPX's design focuses on parallelism than concurrency. Concurrency is defined to have several simultaneously computations and parallelism is simultaneous execution of tasks\cite{r15}. So it enables HPX to have both time and spatial parallelization \cite{r19} due to using \textit{$future$}, which results in having asynchronous task execution. In HPX, asynchronous function execution is the fundamental basic of asynchronous parallelism.

A \textit{$future$} is a computational result that is initially unknown but becomes available at a later time~\cite{r20}. The goal of using \textit{$future$} is to let every computation proceed as far as possible. Using \textit{$future$} enables the continuation of the process without waiting for the results of the previous step to be completed, which eliminates the global barriers at the end of the execution of the parallel loop. \textit{$future$} based parallelization provides rich semantics for exploiting the higher level parallelism available within each application that significantly improves scaling. Figure~\ref{f4} shows the scheme of \textit{$future$} performance with 2 \textit{localities}, where a \textit{locality} is a collection of processing units (PUs) that have access to the same main memory. 


\begin{figure} [!h]
\captionsetup{justification=centering}
\begin{center}
\centering
\includegraphics[width=0.75\columnwidth]{Pictures/f4a.jpg}
\caption {The principle of operation of a \textit{$future$} in HPX}
\label{f4}
\end{center}
\end{figure}



Figure~\ref{f4} shows that until returning the computed value, the other threads do not stop their process. Instead, they continue their process until they need the computation result from previous step. Then, HPX threads requesting suspend until $future$ value is computed. Threads access the $future$ value by performing a \textit{$future$.get()} in Figure~\ref{f4}. When the result becomes available, the \textit{$future$} resumes all HPX suspended threads waiting for the value. It can be seen that this process eliminates the global barrier synchronizations at the end of application parallelization while only those threads that depend on the $future$ value are suspended. With this scheme, HPX allows asynchronous execution of threads.


\section{Airfoil code with HPX}
\label{sec:air}

For our evaluation we chose the Airfoil application presented in \cite{o8}. This model uses an unstructured grid and it consists of five parallel loops: \texttt{save\_soln}, \texttt{adt\_calc}, \texttt{res\_calc}, \texttt{bres\_calc}, \texttt{update}, which \texttt{save\_soln},  and \texttt{update} loops are direct loops and the others are indirect loops. 

The OP2 API provides a parallel loop function allowing the computation over sets. Figure \ref{l1} shows the sequential loops used in Airfoil application within \texttt{airfoil.cpp}. Saving old data values, applying the computation on each data value and updating them are implemented with mentioned five loops. Each loop iterates over a specified set and the operations that is performed with a user's kernels are defined in a header file for each loop: \texttt{save\_soln.h}, \texttt{adt\_calc.h}, \texttt{res\_calc.h}, \texttt{bres\_calc.h} and \texttt{update.h}. 

\begin{figure} [!h]
    \begin{lstlisting}
op_par_loop(save_soln,"save_soln", cells,
  op_arg_dat_0,...,op_arg_dat_n1);
  
op_par_loop(adt_calc,"adt_calc",cells,
  op_arg_dat_0,...,op_arg_dat_n2);
  
op_par_loop(res_calc,"res_calc",edges,
  op_arg_dat_0,...,op_arg_dat_n3);

op_par_loop(bres_calc,"bres_calc",bedges,
  op_arg_dat_0,...,op_arg_dat_n4);
  
op_par_loop(update,"update",cells,
  op_arg_dat_0,...,op_arg_dat_n5);
  
    \end{lstlisting}
    \caption{\small{Five loops are used in \texttt{airfoil.cpp} for saving old data values, applying the computation on each data value and updating them. \texttt{save\_soln},  and \texttt{update} loops are direct loops and the others are indirect one.}}
    \label{l1}
\end{figure}


Figure \ref{l2} shows the loop part of \texttt{adt\_calc} function parsed with \texttt{op\_par\_loop}, which illustrates how each cell updates its data value with using the data values passed to \texttt{adt\_calc}: \texttt{blockId}, \texttt{offset\_b} and \texttt{nelem}. The value of \texttt{blockId} is defined based on the value of \texttt{blockIdx} used in OP2 API.  \texttt{offset\_b} and \texttt{nelem} are defined based on the value of \texttt{blockId}. The arguments are passed to the \texttt{adt\_calc} user kernel subroutine, which does the computation for each and every iteration in the inner loop from \texttt{offset\_b} to \texttt{offset\_b+nelem}. More details about the Airfoil application and its computation process can be found in \cite{o8}.

\begin{figure} [!h]
    \begin{lstlisting}
#pragma omp parallel for
for(int blockIdx=0; blockIdx<nblocks; blockIdx++){
  int blockId  = //defined with blockIdx in OP2 API
  int nelem    = //defined based on blockId 
  int offset_b = //defined based on blockId
        
  for ( int n=offset_b; n<offset_b+nelem; n++ ){
   .
   .
   .

   adt_calc(...);
  }
}
  
    \end{lstlisting}
    \caption{\small{\texttt{\#pragma omp parallel for} is used for loop parallelization in OP2 for Airfoil application to obtain the loop parallelization on one node and also on the distributed nodes using MPI.}}
    \label{l2}
\end{figure}


As shown in Figure \ref{l2}, \texttt{\#pragma omp parallel for} is used for each loops passed with \texttt{op\_par\_loop} in OP2 for loop parallelization, in both having one node or having distributed nodes. However, scalability is limited due to the sequential time between the parallel loops as described by Amdahl's Law that scaling is limited by the amount of sequential time caused by the implicit barrier in the fork-join model \cite{r23}. HPX parallelization methods are used here instead of OpenMP to achieve an optimal parallelization for the loops parsed with OP2. 

In this research we use two HPX methods for loop parallelization: I) using \texttt{hpx::parallel::for\_each} with parallel policy and II) using \texttt{hpx::async}. First method is described in more details in section \ref{sec:for} and second method is explained in section \ref{sec:async}. The comparison results of these two methods and OpenMP can be found in section \ref{sec:res}.

\subsection{\texttt{hpx::parallel::for\_each}}
\label{sec:for}

In this method, for the loops exist within Airfoil kernel functions, we implement one of the execution policies from HPX to make the loops to execute in parallel. The list of the execution policies can be found in \cite{hpx_v0.9.11}. 

We use \texttt{par} policy in implementing \texttt{for\_each} in this section. We were able to modify OP2 source-to-source translator with Python to automatically produce \texttt{hpx::parallel::for\_each} instead of using \texttt{\#pragma omp parallel for} for a loop parallelization. Figure \ref{l3} shows the part of code related to \texttt{adt\_calc} function parsed with \texttt{op\_par\_loop}. In this method \texttt{airfoil.cpp} remains the same as Figure \ref{l1}.

\begin{figure} [!h]
    \begin{lstlisting}    
auto r=boost::irange(0, nblocks);
hpx::parallel::for_each(hpx::parallel::par,
  r.begin(), r.end(),[&](std::size_t blockIdx){
  
  int blockId  = //defined with blockIdx in OP2 API
  int nelem    = //defined based on blockId 
  int offset_b = //defined based on blockId
        
  for ( int n=offset_b; n<offset_b+nelem; n++ ){
   .
   .
   .

   adt_calc(...);
  }
});
  
    \end{lstlisting}
    \caption{\small{Implementing \texttt{hpx::parallel::for\_each} for loop parallelization in OP2 for Airfoil application. HPX is able to control a grain size in this method. As a result, it helps in reducing processor starvation caused by the fork-join barrier at the end of the execution of the parallel loop.}}
    \label{l3}
\end{figure}



This example exposes the same disadvantage as OpenMP, which is a representation of fork-join parallelism that introduces the global barriers at the end of the loop. But the difference of this method with OpenMP is that using \texttt{hpx::parallel::for\_each} is able to control a grain size.  Grain size is the amount of works per threads. As discussed in section \ref{sec:hpx}, HPX enables fine-grained task parallelism and determines the grain size as small as possible to give task to all available threads. \texttt{hpx::parallel::for\_each} helps creating sufficient amount of parallelism, which is determined from auto-partitioner algorithm in HPX estimated at runtime that how many iterations will run on the same thread. As a results, \texttt{hpx::parallel::for\_each} with \texttt{par} policy helps in reducing processor starvation caused by the fork-join barrier at the end of the execution of the parallel loop. The experimental result is discussed in section \ref{sec:res}.

Moreover, if the function \texttt{adt\_calc} schedules more parallel work, the computation time will be more than the communication overhead, which helps to obtain higher parallelism level.



%However, for the smaller loops it has less communication overhead than hpx::async 








\subsection{\texttt{hpx::async}}
\label{sec:async}
%The most compute intensive loop res_calc has about 100 floating-point operations performed per mesh edge and is called 2000 times during total execution of the application.

Here, we implement \texttt{hpx::async} for loop parallelization that makes the invocation of the loop asynchronous. Asynchronous task execution means that a new HPX-thread will be scheduled. The call to \texttt{async} provides a new $future$ instance which represents the result of the function execution.  

In Figure \ref{l5}, the function \texttt{work} in Figure \ref{l5a} is called in each iteration and the returned $future$ is stored in \texttt{new\_data} in Figure \ref{l5b}, which allows the asynchronization within the executed loop implemented by \texttt{hpx::async}. OP2 source-to-source translator with Python is modified here and \texttt{hpx::async} is automatically produced for each loop within Airfoil application instead of \texttt{\#pragma omp parallel for}.
 
 
\begin{figure} [!h]
    \begin{subfigure}[b]{0.5\textwidth}
    \begin{lstlisting}
void work(int offset_b, int nelem,
  op_arg arg0,...,op_arg argn2){
     
  for ( int n=offset_b; n<offset_b+nelem; n++ ){
   .
   .
   .

    adt_calc(...);
  }  
}

    \end{lstlisting}
            \caption{\small{Function \texttt{work} is called for each iteration and returns results as a $future$.}}
            \label{l5a}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.5\textwidth}
    \begin{lstlisting}
for(int blockIdx=0; blockIdx<nblocks; blockIdx++){
  int blockId  = //defined with blockIdx in OP2 API
  int nelem    = //defined based on blockId 
  int offset_b = //defined based on blockId

  new_data.push_back(hpx::async(work,offset_b,nelem,
  arg_0,...,arg_m2));
}

    \end{lstlisting}
     \caption{\small{Storing the returned $future$ from \texttt{work} in \texttt{new\_data}.}}
     \label{l5b}
    \end{subfigure}
    \caption{\small{Implementing \texttt{hpx::async} for loop parallelization in OP2 for Airfoil application. It makes the invocation of the loop asynchronous and return $future$, which is is stored in \texttt{new\_data}. \texttt{hpx::async} allows the asynchronization within the executed loop.}}
    \label{l5}
\end{figure}

In this method \texttt{airfoil.cpp} is changed as shown Figure \ref{l4}. Each kernel function within \texttt{op\_par\_loop} returns its $future$ results as \texttt{new\_data}. Then, \texttt{hpx::wait\_all} is used which waits for a \texttt{new\_data} computation to be completed after each \texttt{op\_par\_loop}, since the computation in each loop depends on the previous loop. However, we can change the place of \texttt{hpx::wait\_all} within a code if there is no need to wait for the previous loop to complete its computation. This gives an ability to control a asynchronous task execution. The caller \texttt{hpx::wait} for the function provides the result of the function execution. So, synchronously task execution suspends the current thread executing the process and the thread will be rescheduled once the function is completely executed. 

\begin{figure}
    \begin{lstlisting}
new_data1.push_back(
op_par_loop(save_soln,"save_soln", cells,
  op_arg_dat_0,...,op_arg_dat_n1);
hpx::wait_all(new_data1);
  
new_data2.push_back(
op_par_loop(adt_calc,"adt_calc",cells,
  op_arg_dat_0,...,op_arg_dat_n2);
hpx::wait_all(new_data2);
   
new_data3.push_back(
op_par_loop(res_calc,"res_calc",edges,
  op_arg_dat_0,...,op_arg_dat_n3);
hpx::wait_all(new_data3);
 
new_data4.push_back(
op_par_loop(bres_calc,"bres_calc",bedges,
  op_arg_dat_0,...,op_arg_dat_n4);
 hpx::wait_all(new_data4);
   
new_data5.push_back(
op_par_loop(update,"update",cells,
  op_arg_dat_0,...,op_arg_dat_n5);
hpx::wait_all(new_data5);
   
    \end{lstlisting}
    \caption{\small{\texttt{airfoil.cpp} is changed for using \texttt{hpx::async} for loop parallelization in OP2. \texttt{new\_data} is returned from each kernel function after calling \texttt{op\_par\_loop} and \texttt{hpx::wait\_all} is used after each loop till completing the results related to the previous loops.}}
    \label{l4}
\end{figure}



The goal of using $future$ is to let the computation within a loop as far as possible and avoid a global barrier synchronization forced with using  \texttt{hpx::parallel::for\_each} and \texttt{\#pragma omp parallel for}. As a result, using $futures$ allows the continuation of the current computations without waiting for the computations of the previous step, if their results are not needed in the current step. Removing global barrier synchronizations improves HPX performance, but for the larger amount of work. For the smaller size of work, the communication overhead increases due to thread competition for shared resources, which inhibits achieving an optimal scalability. The experimental result is discussed in more details in section \ref{sec:res}.


\section{Experimental Results}
\label{sec:res}


The experiments in this research have been executed on a 32 core system with 2 sockets. The main OS used by the shared memory system is 64 bit Linux Mint 17.2. The OpenMP linking is done through the version of OpenMP primitives available in the GNU g++ compilers version 5.1.0. The HPX version 0.9.10~\cite{hpx_v0.9.11} is used here.
 
 

For evaluating HPX performance for loop parallelization produced with OP2, we perform strong scaling experiments, which the problem size is kept the same as the number of threads increases. Figure \ref{f1} shows the strong scaling for three loop parallelization methods: \texttt{\#pragma omp parallel for}, \texttt{hpx::parallel::for\_each} and \texttt{hpx::async} for Airfoil application.

\begin{figure} [!h]
\begin{center}
\centering
\includegraphics[width=1\columnwidth]{Pictures/op2hpx1.jpg}
\caption {\small{Comparison results of strong scaling between \texttt{\#pragma omp parallel for}, \texttt{hpx::parallel::for\_each} and \texttt{hpx::async} used for Airfoil application,
with up to 64 threads. The results illustrate a better performance by \texttt{hpx::parallel::for\_each} for the larger number of threads, which is due to control a grain size with HPX and as a result reducing a resource starvation.}}
\label{f1}
\end{center}
\end{figure}

By considering the above results, we can see the improvement in the performance over the OP2 (initial) version. In Figure \ref{f1}, it is illustrated that \texttt{hpx::parallel::for\_each} perform better for Airfoil application than the other two methods and the reason is related to the managing grain size within a loop in this method. For $32$ threads, \texttt{hpx::parallel::for\_each} improves by about 35\% and \texttt{hpx::async} improves by about 12\% in scaling compared to \texttt{\#pragma omp parallel for}. 

We expected the better performance for \texttt{hpx::async} as it removes a global barrier synchronization and enables asynchronous task execution. However as it is shown in Figure \ref{l4}, \texttt{hpx::wait\_all} is used after each loop which produces a barrier as well.  So these loop dependencies in Airfoil application avoid having higher parallelism level. Also, when the problem size is large enough, there will be enough work for all threads, which hides the communication latencies behind useful work. So for the larger problem size, the more parallelism can be extracted from the application which results to better parallel efficiency. 




\section{Conclusion and Future Works}
\label{sec:future}

The work presented in this paper shows how the HPX runtime system can be used to implement C++ application frameworks. We changed the OP2 python source-to-source translator without changing OP2 API to automatically use HPX for loop parallelization within a code generated by OP2. Airfoil simulation written in OP2 is used to compare the HPX performance with OpenMP that is used in OP2 parallel loops. We are able to obtain 35\% scalability improvement in using \texttt{hpx::parallel::for\_each} and 12\% scalability improvement in using \texttt{hpx::async} for loop parallelization compared with OpenMP.

With using \texttt{hpx::parallel::for\_each}, HPX is able to control a grain size and as a result a resource starvation is reduced as well. However, using \texttt{hpx::parallel::for\_each} introduces the global barriers at the end of the loop same as OpenMP, which inhibits having a desired scalability. On the other hand, \texttt{hpx::async} allows the computation within a loop to be processed as far as possible, which avoids a global barrier synchronization. \texttt{hpx::async} gives a capability of automatically interleaving consecutive loops, which they do not need to wait for the previous loops to be completed. So \texttt{hpx::wait\_all} is needed to be used once at the end of all these loops and as a result, the other unnecessary global barrier between each consecutive loops will be removed as well. 

Interleaving execution of $direct$ loops can be done during a compile-time, however it is almost difficult to interleave $indirect$ loops during a compile-time. Using $future$ based techniques in HPX such as \texttt{hpx::async} enables having an $indirect$ loop interleaving during a run-time. But it requires to know a loop dependency, which can be provided with OP2. 

In OP2, \texttt{op\_par\_loop} describes the loop over the set cells and makes an explicit indication that how each argument within a loop accessed. This argument indication can be used to determine independent loops, those can be run in parallel. For a future work, improving a parallel execution scalability by automatically removing unnecessary global barriers using \texttt{hpx::async} and OP2 will be studied. 


\bibliography{References}
\bibliographystyle{IEEEtran}


\end{document}

